---
layout: post
comments: true
categories: 机器学习
---
&emsp;&emsp;总结几个常用的文本分类器，从数学原理到效果测评。

#### 一、常用文本分类器
##### 1、朴素贝叶斯分类器

$$ P(Y|X)=\frac{P(Y)P(X|Y)}{P(X)} $$

P(Y\|X)是已知X发生后Y的条件概率，也由于得自X的取值而被称作Y的后验概率。

P(Y)是Y的先验概率（或边缘概率）。之所以称为"先验"是因为它不考虑任何X方面的因素。
在文本分类中，Y代表文本所属类别，X代表出现的文本。分类器解决的问题是，寻找最大的P(Y\|X)，即Y在给定X下的最大后验概率。

注意公式中的分母部分，在给定X条件下，与Y无关，所以只需要比较分子部分P(Y)P(X\|Y)即可。其中P(X\|Y)称为似然项。假设短文本长度为N（X的维度）,词表大小为M（X每一维的取值），类别数为K(Y的取值)。整体的组合复杂度为$ K*M^{N} $

复杂度过高，所以引入条件独立性假设，各个词之间独立出现，不会相互影响。这样似然项可以改写为P((X1,X2,...Xm)\|Y) ,如果不考虑位置，复杂度降为K*M。在实际使用中，还要考虑由于数据不足导致的某个词在某个类别下没有出现的情况，加入拉普拉斯平滑项。

举一个实际的例子，出自信息检索导论

类别：短文本

Chine: Chinese Beijing Chinese

|类别|短文本|
|:---:|:---:|
|Chine| Chinese Beijing Chinese |
|Chine| Chinese Chinese Shanghai|
|Chine| Chinese Macao|
|Japan| Tokyo Japanese Chinese|
|?| Chinese Chinese Chinese Tokyo Japanese|

根据前四条训练数据，判断最后一条数据所属类别。首先求各个类别的先验概率：

P(Chine)=3/4
p(Japan)1/4
然后统计加入拉普拉斯平滑后的各个词在各个类别下的后验概率：

P(Chinese\|Chine) = (5+1)/(8+6) = 3/7    
p(Beijing\|Chine) = (1+1)/(8+6) = 1/7    
P(Shanghai\|Chine) = (1+1)/(8+6) = 1/7    
p(Macao\|Chine) = (1+1)/(8+6) = 1/7    
p(Japanese\|Chine) = (0+1)/(8+6) = 1/14    
p(Tokyo\|Chine) = (0+1)/(8+6) = 1/14   

P(Chinese\|Japan) = (1+1)/(3+6) = 2/9    
p(Beijing\|Japan) = (0+1)/(3+6) = 1/9    
P(Shanghai\|Japan) = (0+1)/(3+6) = 1/9    
p(Macao\|Japan) = (0+1)/(3+6) = 1/9    
p(Japanese\|Japan) = (1+1)/(3+6) = 2/9    
p(Tokyo\|Japan) = (1+1)/(3+6) = 2/9    

测试数据在各个类别下概率计算：
$$
\begin{aligned}
P(China|Chinese Chinese Chinese Tokyo Japanese) &=P(China)P(Chinese|China)^{3}P(Tokyo|China)P(Japanese|China)\\
&=\frac{3}{4}\ast (\frac{3}{7})^{3}\ast \frac{1}{14}\ast \frac{1}{14}
\end{aligned}
$$

$$
\begin{aligned}
P(Japan|Chinese Chinese Chinese Tokyo Japanese)&=P(Japan)P(Chinese|Japan)^{3}P(Tokyo|Japan)P(Japanese|Japan)\\
&=\frac{1}{4}\ast (\frac{2}{9})^{3}\ast \frac{2}{9}\ast \frac{2}{9}
\end{aligned}
$$

测试数据在Chine类下的概率为0.0003，在Japan类的概率为0.0001，所以会分到Chine类下。

#### 2、fastText
fastText是facebook提出的一个文本分类器，其特点是实现简单，训练速度快。在普通cpu上，fastText可以在10分钟内训练10亿条数据，在1分钟内预测50万条数据分类。（类别种类30万）

fastText的网络结构和word2vec的CBOW十分相似，只是把中间词换成了类别标签。其网络结构如下：
![](http://ww1.sinaimg.cn/large/75e7ad61ly1fv0125naagj20go08aaa9.jpg)

fastText的目标函数可以写为：

$$ Obj.=min(-\frac{1}{N}\sum_{n=1}^{N}y_{n}log(f(BAx_{n})))
$$

其中，x为输入词的向量均值，y为分类的label，应该是一个多维one hot编码向量。整个网络只有简单的三层结构。

从目标函数可以看出，算法是利用极大似然估计法，求解参数，使得全体样本正确分类的概率最大，最后的output层可以添加softmax激活函数。

但是，这个简单的模型却有一个问题，那就是如果需要分类的类别过多，会使得模型每轮迭代需要更新的参数过多，影响训练速度。

作者采用了和CBOW一样的思想，对输出层的label进行huffman编码，利用hierarchical softmax方法，降低模型计算复杂度。

假设一共有k个类别，hidden输出向量维度为h(一般取100)，原训练方式每次迭代需要更新的参数为O(kh)，使用hierarchical softmax后，更新的需要更新的参数将为O(hlog(k))，注意，模型整体的参数总数并没有减少，只是每次迭代需要更新的参数减少了。

因为模型采用的是词袋模型，没有考虑词之间的先后顺序，在实际使用中，可采用n-gram模型，增加局部词的空间信息，一般情况下使用3-gram。

最终，作者在两类任务上(情感分型和标签预测)给出了测评。最终结果显示，与其他的方法相比，fastText在准确率上与其他方法基本持平，但是在训练时间上有十分明显的优势，这也是这个模型最大的亮点-fast。

##### 3、textCNN
textCNN也是一个经典的base文本分类器，在2014年由New York大学的Yoon Kim提出，主要思想是利用CNN网络和预训练好的词向量进行文本分类。

textCNN提出的网络模型（其中一个）如下：
![](http://ww1.sinaimg.cn/large/75e7ad61ly1fv8x1ofg8dj20tq0ca40s.jpg)

在论文中，作者共提出了四个模型变种，上图显示的是最复杂的一个，CNN-multichannel，这里的multichannel指的是第一层的两个通道。具体四个模型如下：

1. CNN-rand
2. CNN-static
3. CNN-non-statistic
4. CNN-multichannel

下面详细解释下上面的模型：

第一层是文本表示层或者叫输入层，是一个n*k*2的张量。其中n为词的个数，k为每个词向量对应的维数。第一层还有两个通道，分别对应静态和非静态通道，静态的意思是，词向量在模型训练过程中，不进行更新，非静态的是指模型训练过程中，更新词向量，模型在初始化阶段，两个通道的值是完全相同的。

第二层就是卷积层，卷积核的形状为h*k，注意，这里的h为词向量的维度，也就是说，经过卷积层，输出张量的维度会变为(n-h+1,1,m)，m为卷积核的个数。

第三层为pooling层，每个卷积核的输出分别进行max-pooling操作，输出变为m维向量。

最后一层是全连接层+softmax函数，用于对分类进行预测。

在训练过程中，除了更新模型的参数，也会更新输入层的非静态通道对应的词向量。这就是CNN-multichannel。

其余三个模型基本与这个模型一致，只不过输入变为单通道。
1. CNN-rand: 输入层的词向量并不是预先训练好的，而是随机生成的。在模型训练过程中，同时更新参数和词向量。

2. CNN-static: 输入层词向量是预先训练好的，在模型训练过程中，只更新参数，词向量保持不变。

3. CNN-non-statistic: 输入词向量是预先训练好的，在模型训练过程中，更新词向量。

第一个模型CNN-rand可以算是个基础的base版，在四个模型中效果是最差的，这也从侧面反应了这种网络结构不适合直接训练word-embedding。

第二个模型CNN-static使用预先训练的词向量，但是不更新词向量，训练过程是最简单的，其效果也不错。

第三个模型CNN-non-statistic与第二个模型的唯一区别是，在训练过程中更新词向量，其效果有了明显的提高。

第四个模型CNN-multichannel，在输入层采用了双通道，一个更新词向量，一个不更新词向量。作者交代这样做的目的是，为了对抗模型三中可能产生的过拟合。但是从最终的结果上，模型4和模型3的效果基本持平，没有明显提升。实际使用时，可以考虑使用模型3就行了。

最后放一个四个模型的测评效果图。

![](http://ww1.sinaimg.cn/large/75e7ad61ly1fv91vwy8a5j20xs0ju7bd.jpg)

从实验结果可以看出，在不同的数据集上，textCNN都有不俗的表现。

最终，作者对模型进行了几点总结：
1. 输出层的dropout在正则化上非常有用，可以有效对抗过拟合，经评估，dropout提升了模型效果2%~4%。
2. 高质量的embedding非常有用，对模型效果起着决定性作用。
3. 使用卷积+embedding就可以达到这种效果，说明embedding在NLP领域都有非常重要的作用。
